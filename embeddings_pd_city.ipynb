{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Unsupervised Embedding Methods\n",
    "# ====================================\n",
    "\n",
    "import re\n",
    "import osmnx as ox\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics import f1_score, silhouette_score\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import global_mean_pool, GCNConv\n",
    "import torch.nn as nn\n",
    "\n",
    "# Make sure we have consistent results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_names(graph_codes):\n",
    "    city_names = {}\n",
    "    for code in graph_codes:\n",
    "        try:\n",
    "            place = ox.geocode_to_gdf(f\"{code[1:]}\")  # Убираем 'R'\n",
    "            city_name = place['name'].values[0]\n",
    "            city_names[code] = city_name\n",
    "        except Exception as e:\n",
    "            print(f\"Не удалось загрузить город для {code}: {e}\")\n",
    "            city_names[code] = \"Unknown\"\n",
    "    return city_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Part 1: Overview of Graph Embedding Methods\n",
    "# ====================================\n",
    "\n",
    "\"\"\"\n",
    "Graph embedding methods transform graph structures into vector representations.\n",
    "This enables machine learning algorithms to work with graph data.\n",
    "\n",
    "Types of Graph Embedding Algorithms:\n",
    "1. Node Embedding Algorithms:\n",
    "   - Traditional: PCA, MDS, Laplacian Eigenmaps\n",
    "   - Random Walk-based: DeepWalk, Node2Vec, LINE\n",
    "   - Neural Network-based: GCN, GraphSAGE, GAT\n",
    "   - Matrix Factorization: GraRep, HOPE\n",
    "   - Probabilistic: VGAE, Deep Graph Infomax\n",
    "   - Structural: struc2vec\n",
    "\n",
    "2. Edge Embedding Algorithms:\n",
    "   - Operator-based (Hadamard, average, etc.) on node embeddings\n",
    "   - Explicit edge embedding methods\n",
    "\n",
    "3. Whole Graph Embedding Algorithms:\n",
    "   - Graph Kernels: Weisfeiler-Lehman, Graphlet\n",
    "   - Neural Methods: Graph2Vec, DGCNN, Readout functions in GNNs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Part 2: Graph Data Loading and Conversion\n",
    "# ====================================\n",
    "\n",
    "def generate_random_graph(n, p):\n",
    "    \"\"\"\n",
    "    Generate a random Erdos-Renyi graph and convert to PyTorch Geometric format.\n",
    "    \n",
    "    Args:\n",
    "        n (int): Number of nodes\n",
    "        p (float): Probability of edge creation\n",
    "        \n",
    "    Returns:\n",
    "        Data: PyTorch Geometric graph object\n",
    "    \"\"\"\n",
    "    G = nx.erdos_renyi_graph(n, p)\n",
    "    \n",
    "    # Convert to PyTorch Geometric format\n",
    "    pyg_graph = Data(\n",
    "        x=torch.ones(G.number_of_nodes(), 1),  # Simple node features (all ones)\n",
    "        edge_index=torch.tensor(list(G.edges)).t().contiguous()  # Edge list in COO format\n",
    "    )\n",
    "    \n",
    "    return pyg_graph\n",
    "\n",
    "def load_and_convert_graph(pkl_path, threshold=10):\n",
    "    \"\"\"\n",
    "    Load graph data from pickle file and convert to PyTorch Geometric format.\n",
    "    \n",
    "    The pickle file is expected to contain a dictionary with 'dgms' key,\n",
    "    which contains distance and coordinate information.\n",
    "    \n",
    "    Args:\n",
    "        pkl_path (str): Path to the pickle file\n",
    "        threshold (float): Threshold distance for creating edges\n",
    "        \n",
    "    Returns:\n",
    "        Data: PyTorch Geometric graph object\n",
    "    \"\"\"\n",
    "    # Load data dictionary from pickle\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        data_dict = pickle.load(f)\n",
    "    \n",
    "    # Extract distance and coordinate arrays\n",
    "    distances = data_dict['dgms'][0]   # Distance array\n",
    "    coordinates = data_dict['dgms'][1]  # Coordinate array\n",
    "\n",
    "    # Create empty NetworkX graph\n",
    "    G_net = nx.Graph()\n",
    "    \n",
    "    # Add nodes with coordinates\n",
    "    for i, coord in enumerate(coordinates):\n",
    "        G_net.add_node(i, pos=tuple(coord))\n",
    "    \n",
    "    # Add edges based on distances with threshold\n",
    "    num_nodes = len(distances)\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i + 1, num_nodes):\n",
    "            if distances[i, 1] < threshold and distances[j, 1] < threshold:\n",
    "                G_net.add_edge(i, j, weight=distances[i, 1])\n",
    "    \n",
    "    # Convert to PyTorch Geometric format\n",
    "    if G_net.number_of_edges() > 0:\n",
    "        edge_index = torch.tensor(list(G_net.edges)).t().contiguous()\n",
    "    else:\n",
    "        # Create empty edge tensor if no edges\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    \n",
    "    pyg_data = Data(\n",
    "        x=torch.ones(G_net.number_of_nodes(), 1),  # Simple node features (all ones)\n",
    "        edge_index=edge_index  # Edge list in COO format\n",
    "    )\n",
    "    return pyg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Part 3: Graph Neural Network for Embedding\n",
    "# ====================================\n",
    "\n",
    "class GNNEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Neural Network for generating graph embeddings.\n",
    "    \n",
    "    This model uses a Graph Convolutional Network (GCN) layer followed\n",
    "    by a global pooling operation to create a fixed-size embedding vector\n",
    "    for each graph.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        \"\"\"\n",
    "        Initialize the GNN embedding model.\n",
    "        \n",
    "        Args:\n",
    "            hidden_dim (int): Dimension of the hidden/embedding space\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # GCN layer: transforms node features (dim=1) to hidden_dim\n",
    "        self.conv1 = GCNConv(1, hidden_dim)\n",
    "        \n",
    "        # Pooling function to aggregate node embeddings into graph embeddings\n",
    "        # Alternatives: global_add_pool, global_max_pool\n",
    "        self.pool = global_mean_pool\n",
    "        \n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass to generate graph embeddings.\n",
    "        \n",
    "        Args:\n",
    "            data (Data): PyTorch Geometric graph data object\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Graph embedding vector\n",
    "        \"\"\"\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # Apply GCN layer with ReLU activation\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        \n",
    "        # Pool node embeddings to get graph embedding\n",
    "        return self.pool(x, batch)\n",
    "\n",
    "def get_graph_embedding(graph, model):\n",
    "    \"\"\"\n",
    "    Generate embedding for a single graph.\n",
    "    \n",
    "    Args:\n",
    "        graph (Data): PyTorch Geometric graph data object\n",
    "        model (nn.Module): GNN model for embedding\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Graph embedding vector\n",
    "    \"\"\"\n",
    "    # Set batch indices (single graph: all nodes have batch index 0)\n",
    "    graph.batch = torch.zeros(graph.num_nodes, dtype=torch.long)\n",
    "    \n",
    "    # Generate embedding (no gradient needed)\n",
    "    with torch.no_grad():\n",
    "        emb = model(graph)\n",
    "    \n",
    "    return emb.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Part 4: Graph Data Analysis Pipeline\n",
    "# ====================================\n",
    "\n",
    "def load_graph_dataset(data_dir=\"./output_3\", pattern=\"*/result.pkl\", threshold=10):\n",
    "    \"\"\"\n",
    "    Load all graph data from a directory and convert to PyTorch Geometric format.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Directory containing graph data\n",
    "        pattern (str): Pattern to match pickle files\n",
    "        threshold (float): Threshold for edge creation\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (graph_names, pyg_graphs) - Lists of graph names and PyG objects\n",
    "    \"\"\"\n",
    "    # Find all pickle files matching the pattern\n",
    "    pkl_files = glob.glob(os.path.join(data_dir, pattern))\n",
    "    print(f\"Found {len(pkl_files)} files\")\n",
    "    \n",
    "    # Load graphs and their names\n",
    "    graph_names = []\n",
    "    pyg_graphs = []\n",
    "    \n",
    "    for p in tqdm(pkl_files, desc=\"Loading graphs\"):\n",
    "        try:\n",
    "            graph = load_and_convert_graph(p, threshold=threshold)\n",
    "            pyg_graphs.append(graph)\n",
    "            # Extract graph name from directory name\n",
    "            graph_names.append(os.path.basename(os.path.dirname(p)))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load graph from {p}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully loaded {len(pyg_graphs)} graphs\")\n",
    "    return graph_names, pyg_graphs\n",
    "\n",
    "def compute_embeddings(graph_names, pyg_graphs, hidden_dim=64):\n",
    "    \"\"\"\n",
    "    Compute embeddings for a list of graphs.\n",
    "    \n",
    "    Args:\n",
    "        graph_names (list): List of graph names\n",
    "        pyg_graphs (list): List of PyTorch Geometric graph objects\n",
    "        hidden_dim (int): Dimension of the embedding space\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (embeddings, graph_names_emb, df_emb) - Embedding array, \n",
    "               list of graph names, and distance matrix DataFrame\n",
    "    \"\"\"\n",
    "    # Initialize GNN model\n",
    "    model = GNNEmbedder(hidden_dim=hidden_dim)\n",
    "    \n",
    "    # Compute embeddings\n",
    "    embeddings = []\n",
    "    graph_names_emb = []\n",
    "    \n",
    "    for name, pyg_graph in tqdm(zip(graph_names, pyg_graphs), \n",
    "                                desc=\"Computing embeddings\", \n",
    "                                total=len(graph_names)):\n",
    "        try:\n",
    "            emb = get_graph_embedding(pyg_graph, model)\n",
    "            if emb.size == 0 or emb.shape[0] == 0:\n",
    "                print(f\"Graph {name} returned empty embedding. Using zero vector.\")\n",
    "                emb = np.zeros((1, hidden_dim))\n",
    "            embeddings.append(emb)\n",
    "            graph_names_emb.append(name)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to compute embedding for {name}: {e}\")\n",
    "    \n",
    "    # Stack embeddings into a single array\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    print(f\"Embedding shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Compute Euclidean distance matrix\n",
    "    emb_distance_matrix = euclidean_distances(embeddings)\n",
    "    df_emb = pd.DataFrame(emb_distance_matrix, \n",
    "                         index=graph_names_emb, \n",
    "                         columns=graph_names_emb)\n",
    "    \n",
    "    return embeddings, graph_names_emb, df_emb\n",
    "\n",
    "def analyze_graph_statistics(pyg_graphs):\n",
    "    \"\"\"\n",
    "    Analyze basic statistics of the graph dataset.\n",
    "    \n",
    "    Args:\n",
    "        pyg_graphs (list): List of PyTorch Geometric graph objects\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of statistics\n",
    "    \"\"\"\n",
    "    # Count nodes for each graph\n",
    "    node_counts = [graph.num_nodes for graph in pyg_graphs]\n",
    "    \n",
    "    # Compute statistics\n",
    "    stats = {\n",
    "        \"mean_nodes\": np.mean(node_counts),\n",
    "        \"median_nodes\": np.median(node_counts),\n",
    "        \"std_nodes\": np.std(node_counts),\n",
    "        \"min_nodes\": np.min(node_counts),\n",
    "        \"max_nodes\": np.max(node_counts)\n",
    "    }\n",
    "    \n",
    "    # Plot node count histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(node_counts, bins=20, edgecolor='black')\n",
    "    plt.xlabel('Number of nodes')\n",
    "    plt.ylabel('Number of graphs')\n",
    "    plt.title('Distribution of graph sizes (node counts)')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Part 5: Graph Clustering and Evaluation\n",
    "# ====================================\n",
    "\n",
    "def cluster_and_evaluate(embedding_data, persistent_diagram_data, n_clusters=2):\n",
    "    \"\"\"\n",
    "    Cluster graphs using both embedding and persistent diagram data,\n",
    "    then evaluate the alignment between the two clustering approaches.\n",
    "    \n",
    "    Args:\n",
    "        embedding_data (DataFrame): Distance matrix from graph embeddings\n",
    "        persistent_diagram_data (DataFrame): Distance matrix from persistent diagrams\n",
    "        n_clusters (int): Number of clusters to create\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with clustering results and evaluation metrics\n",
    "    \"\"\"\n",
    "    # Find common graph names between the two datasets\n",
    "    common_names = sorted(set(embedding_data.index).intersection(\n",
    "                          set(persistent_diagram_data.index)))\n",
    "    print(f\"Found {len(common_names)} common graphs\")\n",
    "    \n",
    "    # Reindex both matrices to have the same order\n",
    "    df_emb_ordered = embedding_data.loc[common_names, common_names]\n",
    "    df_pd_ordered = persistent_diagram_data.loc[common_names, common_names]\n",
    "    \n",
    "    # Use MDS to convert distance matrices to 2D points for visualization\n",
    "    mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "    embedding_2d = mds.fit_transform(df_emb_ordered)\n",
    "    pd_2d = mds.fit_transform(df_pd_ordered)\n",
    "    \n",
    "    # Cluster using KMeans\n",
    "    kmeans_emb = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans_pd = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    \n",
    "    labels_emb = kmeans_emb.fit_predict(embedding_2d)\n",
    "    labels_pd = kmeans_pd.fit_predict(pd_2d)\n",
    "    \n",
    "    # Evaluate clustering quality with silhouette score\n",
    "    sil_emb = silhouette_score(embedding_2d, labels_emb)\n",
    "    sil_pd = silhouette_score(pd_2d, labels_pd)\n",
    "    \n",
    "    # Evaluate agreement between clusterings with F1 score\n",
    "    # Note: This assumes persistent diagram clustering is ground truth\n",
    "    f1 = f1_score(labels_pd, labels_emb, average='weighted')\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        \"common_graphs\": common_names,\n",
    "        \"embedding_2d\": embedding_2d,\n",
    "        \"pd_2d\": pd_2d,\n",
    "        \"labels_emb\": labels_emb,\n",
    "        \"labels_pd\": labels_pd,\n",
    "        \"silhouette_emb\": sil_emb,\n",
    "        \"silhouette_pd\": sil_pd,\n",
    "        \"f1_score\": f1\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def find_optimal_clusters(embedding_2d, pd_2d, max_clusters=5):\n",
    "    \"\"\"\n",
    "    Find optimal number of clusters using silhouette score.\n",
    "    \n",
    "    Args:\n",
    "        embedding_2d (numpy.ndarray): 2D embedding points\n",
    "        pd_2d (numpy.ndarray): 2D persistent diagram points\n",
    "        max_clusters (int): Maximum number of clusters to try\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (optimal_k_emb, optimal_k_pd, scores_emb, scores_pd)\n",
    "    \"\"\"\n",
    "    scores_emb = {}\n",
    "    scores_pd = {}\n",
    "    \n",
    "    for k in range(2, max_clusters + 1):\n",
    "        # Cluster embeddings\n",
    "        kmeans_emb = KMeans(n_clusters=k, random_state=42)\n",
    "        labels_emb = kmeans_emb.fit_predict(embedding_2d)\n",
    "        score_emb = silhouette_score(embedding_2d, labels_emb)\n",
    "        scores_emb[k] = score_emb\n",
    "        \n",
    "        # Cluster persistent diagrams\n",
    "        kmeans_pd = KMeans(n_clusters=k, random_state=42)\n",
    "        labels_pd = kmeans_pd.fit_predict(pd_2d)\n",
    "        score_pd = silhouette_score(pd_2d, labels_pd)\n",
    "        scores_pd[k] = score_pd\n",
    "    \n",
    "    # Find optimal k (highest silhouette score)\n",
    "    optimal_k_emb = max(scores_emb, key=scores_emb.get)\n",
    "    optimal_k_pd = max(scores_pd, key=scores_pd.get)\n",
    "    \n",
    "    return optimal_k_emb, optimal_k_pd, scores_emb, scores_pd\n",
    "\n",
    "def visualize_clustering(embedding_2d, pd_2d, labels_emb, labels_pd, common_names, city_names):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "    scatter1 = axes[0].scatter(embedding_2d[:, 0], embedding_2d[:, 1], \n",
    "                              c=labels_emb, cmap='viridis', s=50, alpha=0.8)\n",
    "    axes[0].set_title('Graph Embedding Clusters')\n",
    "    axes[0].set_xlabel('Dimension 1')\n",
    "    axes[0].set_ylabel('Dimension 2')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    scatter2 = axes[1].scatter(pd_2d[:, 0], pd_2d[:, 1], \n",
    "                              c=labels_pd, cmap='viridis', s=50, alpha=0.8)\n",
    "    axes[1].set_title('Persistent Diagram Clusters')\n",
    "    axes[1].set_xlabel('Dimension 1')\n",
    "    axes[1].set_ylabel('Dimension 2')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    # Добавляем подписи городов\n",
    "    for i, city in enumerate(city_names):\n",
    "        axes[0].annotate(city, (embedding_2d[i, 0], embedding_2d[i, 1]), fontsize=9, alpha=0.75)\n",
    "        axes[1].annotate(city, (pd_2d[i, 0], pd_2d[i, 1]), fontsize=9, alpha=0.75)\n",
    "\n",
    "    legend1 = axes[0].legend(*scatter1.legend_elements(), title=\"Clusters\")\n",
    "    axes[0].add_artist(legend1)\n",
    "\n",
    "    legend2 = axes[1].legend(*scatter2.legend_elements(), title=\"Clusters\")\n",
    "    axes[1].add_artist(legend2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Part 6: Hybrid Classification Model\n",
    "# ====================================\n",
    "\n",
    "class HybridClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid classifier combining graph embeddings with topological features.\n",
    "    \n",
    "    This model can be used for supervised classification of graphs\n",
    "    based on the combination of GNN embeddings and topological features.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid classifier.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Dimension of input features (embedding + topological)\n",
    "            hidden_dim (int): Dimension of hidden layer\n",
    "            num_classes (int): Number of output classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the classifier.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input features\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Classification logits\n",
    "        \"\"\"\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def compute_network_measures(G):\n",
    "    \"\"\"\n",
    "    Compute basic network measures for a graph.\n",
    "    \n",
    "    Args:\n",
    "        G (networkx.Graph): NetworkX graph\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Array of network measures\n",
    "    \"\"\"\n",
    "    measures = {\n",
    "        \"avg_degree\": np.mean(list(dict(G.degree()).values())),\n",
    "        \"clustering_coeff\": nx.average_clustering(G) if G.number_of_edges() > 0 else 0,\n",
    "        \"diameter\": nx.diameter(G) if nx.is_connected(G) and G.number_of_edges() > 0 else 0,\n",
    "        \"density\": nx.density(G),\n",
    "        \"num_nodes\": G.number_of_nodes(),\n",
    "        \"num_edges\": G.number_of_edges()\n",
    "    }\n",
    "    return np.array(list(measures.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Part 7: Main Execution Pipeline\n",
    "# ====================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function for the graph embedding and analysis pipeline.\n",
    "    \"\"\"\n",
    "    print(\"Starting graph embedding and analysis pipeline...\")\n",
    "    \n",
    "    # 1. Load graph data\n",
    "    print(\"\\n1. Loading graph data...\")\n",
    "    graph_names, pyg_graphs = load_graph_dataset(threshold=10)\n",
    "    \n",
    "    # Извлекаем коды R[number]\n",
    "    graph_codes = [re.search(r'R\\d+', name).group() for name in graph_names]\n",
    "    city_names_dict = get_city_names(graph_codes)\n",
    "    city_names = [city_names_dict[code] for code in graph_codes]\n",
    "\n",
    "\n",
    "    \n",
    "    # 2. Analyze graph statistics\n",
    "    print(\"\\n2. Analyzing graph statistics...\")\n",
    "    stats = analyze_graph_statistics(pyg_graphs)\n",
    "    print(f\"Graph statistics:\\n{stats}\")\n",
    "    \n",
    "    # 3. Compute graph embeddings\n",
    "    print(\"\\n3. Computing graph embeddings...\")\n",
    "    embeddings, graph_names_emb, df_emb = compute_embeddings(\n",
    "        graph_names, pyg_graphs, hidden_dim=64)\n",
    "    \n",
    "    # 4. Load persistent diagram distances (Assuming this file exists)\n",
    "    print(\"\\n4. Loading persistent diagram distances...\")\n",
    "    try:\n",
    "        # with open('output_3/bottleneck_distances.pkl', 'rb') as f:\n",
    "            # pd_distance_matrix = pickle.load(f)\n",
    "        pd_distance_matrix = pd.read_csv('output_3/bottleneck_matrix.csv', index_col=0)\n",
    "        print(\"Successfully loaded persistent diagram distances\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: bottleneck_distances_matrix.pkl not found.\")\n",
    "\n",
    "    \n",
    "    # 5. Cluster and evaluate\n",
    "    print(\"\\n5. Clustering and evaluating...\")\n",
    "    results = cluster_and_evaluate(df_emb, pd_distance_matrix, n_clusters=2)\n",
    "    \n",
    "    # 6. Find optimal number of clusters\n",
    "    print(\"\\n6. Finding optimal number of clusters...\")\n",
    "    opt_k_emb, opt_k_pd, scores_emb, scores_pd = find_optimal_clusters(\n",
    "        results[\"embedding_2d\"], results[\"pd_2d\"])\n",
    "    print(f\"Optimal clusters (embedding): {opt_k_emb}\")\n",
    "    print(f\"Optimal clusters (persistent diagrams): {opt_k_pd}\")\n",
    "    print(f\"Silhouette scores (embedding): {scores_emb}\")\n",
    "    print(f\"Silhouette scores (persistent diagrams): {scores_pd}\")\n",
    "    \n",
    "    # 7. Visualize clusters\n",
    "    print(\"\\n7. Visualizing clusters...\")\n",
    "    visualize_clustering(\n",
    "        results[\"embedding_2d\"], \n",
    "        results[\"pd_2d\"], \n",
    "        results[\"labels_emb\"], \n",
    "        results[\"labels_pd\"], \n",
    "        results[\"common_graphs\"], \n",
    "        city_names\n",
    "    )\n",
    "\n",
    "    \n",
    "    print(\"\\nPipeline completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "e0gFQQGsvah3"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
